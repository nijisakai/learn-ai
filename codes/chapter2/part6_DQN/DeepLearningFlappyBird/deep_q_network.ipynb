{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append(\"game/\")\n",
        "import wrapped_flappy_bird as game\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "GAME = 'bird' # the name of the game being played for log files\n",
        "ACTIONS = 2 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVE = 10000. # timesteps to observe before training\n",
        "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev = 0.01)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.01, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, stride):\n",
        "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
        "\n",
        "def createNetwork():\n",
        "    # network weights\n",
        "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
        "    b_conv1 = bias_variable([32])\n",
        "\n",
        "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
        "    b_conv2 = bias_variable([64])\n",
        "\n",
        "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
        "    b_conv3 = bias_variable([64])\n",
        "\n",
        "    W_fc1 = weight_variable([1600, 512])\n",
        "    b_fc1 = bias_variable([512])\n",
        "\n",
        "    W_fc2 = weight_variable([512, ACTIONS])\n",
        "    b_fc2 = bias_variable([ACTIONS])\n",
        "\n",
        "    # input layer\n",
        "    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
        "\n",
        "    # hidden layers\n",
        "    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
        "    h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
        "    #h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
        "    #h_pool3 = max_pool_2x2(h_conv3)\n",
        "\n",
        "    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
        "    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
        "\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
        "\n",
        "    # readout layer\n",
        "    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "\n",
        "    return s, readout, h_fc1\n",
        "\n",
        "def trainNetwork(s, readout, h_fc1, sess):\n",
        "    # define the cost function\n",
        "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
        "    y = tf.placeholder(\"float\", [None])\n",
        "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
        "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
        "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
        "\n",
        "    # open up a game state to communicate with emulator\n",
        "    game_state = game.GameState()\n",
        "\n",
        "    # store the previous observations in replay memory\n",
        "    D = deque()\n",
        "\n",
        "    # printing\n",
        "    a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
        "    h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
        "\n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "    do_nothing = np.zeros(ACTIONS)\n",
        "    do_nothing[0] = 1\n",
        "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
        "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
        "\n",
        "    # saving and loading networks\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
        "    # if checkpoint and checkpoint.model_checkpoint_path:\n",
        "    #     saver.restore(sess, checkpoint.model_checkpoint_path)\n",
        "    #     print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
        "    # else:\n",
        "    #     print(\"Could not find old network weights\")\n",
        "\n",
        "    # start training\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    t = 0\n",
        "    while \"flappy bird\" != \"angry bird\":\n",
        "        # choose an action epsilon greedily\n",
        "        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
        "        a_t = np.zeros([ACTIONS])\n",
        "        action_index = 0\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[random.randrange(ACTIONS)] = 1\n",
        "            else:\n",
        "                action_index = np.argmax(readout_t)\n",
        "                a_t[action_index] = 1\n",
        "        else:\n",
        "            a_t[0] = 1 # do nothing\n",
        "\n",
        "        # scale down epsilon\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        # run the selected action and observe next state and reward\n",
        "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
        "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
        "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
        "        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        # only train if done observing\n",
        "        if t > OBSERVE:\n",
        "            # sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            # get the batch variables\n",
        "            s_j_batch = [d[0] for d in minibatch]\n",
        "            a_batch = [d[1] for d in minibatch]\n",
        "            r_batch = [d[2] for d in minibatch]\n",
        "            s_j1_batch = [d[3] for d in minibatch]\n",
        "\n",
        "            y_batch = []\n",
        "            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
        "            for i in range(0, len(minibatch)):\n",
        "                terminal = minibatch[i][4]\n",
        "                # if terminal, only equals reward\n",
        "                if terminal:\n",
        "                    y_batch.append(r_batch[i])\n",
        "                else:\n",
        "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
        "\n",
        "            # perform gradient step\n",
        "            train_step.run(feed_dict = {\n",
        "                y : y_batch,\n",
        "                a : a_batch,\n",
        "                s : s_j_batch}\n",
        "            )\n",
        "\n",
        "        # update the old values\n",
        "        s_t = s_t1\n",
        "        t += 1\n",
        "\n",
        "        # save progress every 10000 iterations\n",
        "        if t % 10000 == 0:\n",
        "            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX %e\" % np.max(readout_t))\n",
        "        # write info to files\n",
        "        '''\n",
        "        if t % 10000 <= 100:\n",
        "            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
        "            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
        "            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
        "        '''\n",
        "\n",
        "def playGame():\n",
        "    sess = tf.compat.v1.InteractiveSession()\n",
        "    s, readout, h_fc1 = createNetwork()\n",
        "    trainNetwork(s, readout, h_fc1, sess)\n",
        "\n",
        "def main():\n",
        "    playGame()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}